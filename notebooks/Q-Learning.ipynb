{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-26 23:44:53,867] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "# env = gym.make('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize $V(s)$ arbitrarily\n",
    "- Repeat for each episode\n",
    "- Initialize s\n",
    "- Repeat (for each step of episode)\n",
    "- -    $\\alpha \\leftarrow$ action given by $\\pi$ for $s$\n",
    "- - Take action a, observe reward r, and next state s'\n",
    "- - $V(s) \\leftarrow V(s) + \\alpha [r + \\gamma V(s') - V(s)]$ \n",
    "- - $s \\leftarrow s'$\n",
    "- until $s$ is terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value function $Q(s,a)$ defines how good it is to take action $a$ when in state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-26 23:44:54,154] Making new env: Acrobot-v1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Acrobot-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09762701,  0.43037873,  0.20552675,  0.08976637, -1.91876417,\n",
       "        8.25011773])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-466a8ee1b62a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Initialize table with all zeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Set learning parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.85\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#Initialize table with all zeros\n",
    "Q = np.zeros([ env.observation_space.n, env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .85\n",
    "y = .99\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    rAll = 0\n",
    "    d = False\n",
    "    j = 0\n",
    "    # The Q-Table learning algorithm\n",
    "    while j < 99:\n",
    "        j+=1\n",
    "        \n",
    "        #Choose an action by greedily (with noise) picking from Q table\n",
    "        a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
    "        \n",
    "        #Get new state and reward from environment\n",
    "        s1,r,d,_ = env.step(a)\n",
    "        \n",
    "        #Update Q-Table with new knowledge\n",
    "        Q[s,a] = Q[s,a] + lr*(r + y*np.max(Q[s1,:]) - Q[s,a])\n",
    "        \n",
    "        rAll += r\n",
    "        s = s1\n",
    "        if d == True:\n",
    "            break\n",
    "    #jList.append(j)\n",
    "    rList.append(rAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Final Q-Table Values\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Score over time: \" +  str(sum(rList)/num_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/miguel/Jottacloud/HedgingRL\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.envs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gym.spaces.Tuple?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gym.envs.register(id='bs-v3',\n",
    "                  entry_point='gym_bs.envs:EuropeanOptionEnv',\n",
    "                  kwargs={'t': 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gym.wrappers.Monitor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-27 12:06:33,444] Making new env: bs-v3\n",
      "[2017-04-27 12:06:33,831] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('bs-v3')\n",
    "env = gym.wrappers.Monitor(env, \"/tmp/gym-results/bs-v3\", video_callable=False, write_upon_reset=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()\n",
    "# env.action_space.contains(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Q = np.zeros([ env.observation_space.n, env.action_space.n])\n",
    "# Set learning parameters\n",
    "lr = .85\n",
    "y = .99\n",
    "num_episodes = 2000\n",
    "#create lists to contain total rewards and steps per episode\n",
    "#jList = []\n",
    "rList = []\n",
    "for i in range(num_episodes):\n",
    "    #Reset environment and get first new observation\n",
    "    s = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        a = env.action_space.sample()\n",
    "        state, reward, done, _ = env.step(a)\n",
    "    print(\"=======\", reward)\n",
    "#     if i>2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "import pickle\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/tmp/gym-results/bs-v3/openaigym.episode_batch.2.8920.stats.json') as f: x = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.plt.plot(env.underlying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_episode(policy, env, num_steps, render=False):\n",
    "    total_rew = 0\n",
    "    ob = env.reset()\n",
    "    for t in range(num_steps):\n",
    "        a = policy.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: \n",
    "            # print \"the game has been done.\"\n",
    "            break\n",
    "    return total_rew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.stock * s1[0] + env.cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.option.calc(0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'delta': 0.5, 'gamma': 0, 'npv': 0.0, 'theta': 0, 'vega': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from blackscholes import blackScholes\n",
    "blackScholes(0, 1, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeterministicContinuousActionLinearPolicy(object):\n",
    "    \"\"\"\n",
    "    Taken from https://gym.openai.com/evaluations/eval_sXJlX4GVQouaTYTkWemOA\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, theta, ob_space, ac_space):\n",
    "        \"\"\"\n",
    "        dim_ob: dimension of observations\n",
    "        dim_ac: dimension of action vector\n",
    "        theta: flat vector of parameters\n",
    "        \"\"\"\n",
    "        self.ac_space = ac_space\n",
    "        dim_ob = ob_space.shape[0]\n",
    "        dim_ac = ac_space.shape[0]\n",
    "        print(theta, len(theta), dim_ob, dim_ac)\n",
    "        assert len(theta) == (dim_ob + 1) * dim_ac\n",
    "        self.W = theta[0 : dim_ob * dim_ac].reshape(dim_ob, dim_ac)\n",
    "        self.b = theta[dim_ob * dim_ac : None]\n",
    "\n",
    "    def act(self, ob):\n",
    "        a = np.clip(ob.dot(self.W) + self.b, self.ac_space.low, self.ac_space.high)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.37030113  0.24325921 -1.28778306 -2.97953413]\n",
      "[-0.37030113  0.24325921 -1.28778306 -2.97953413] 4 3 1\n",
      "[ 0.43727774 -1.75332178  0.77420752  1.1466121 ]\n",
      "[ 0.43727774 -1.75332178  0.77420752  1.1466121 ] 4 3 1\n",
      "[-0.07263471  3.08798246 -0.99612687  1.414658  ]\n",
      "[-0.07263471  3.08798246 -0.99612687  1.414658  ] 4 3 1\n",
      "[ 0.71772567  0.87022845 -0.01085429 -0.52642814]\n",
      "[ 0.71772567  0.87022845 -0.01085429 -0.52642814] 4 3 1\n",
      "[ 0.37001059 -0.31940614 -0.21332629 -0.19606673]\n",
      "[ 0.37001059 -0.31940614 -0.21332629 -0.19606673] 4 3 1\n",
      "[ 0.80350664  0.77927668  0.02543269 -2.02610253]\n",
      "[ 0.80350664  0.77927668  0.02543269 -2.02610253] 4 3 1\n",
      "[-0.0859375   0.43071566 -0.2717998   0.79093699]\n",
      "[-0.0859375   0.43071566 -0.2717998   0.79093699] 4 3 1\n",
      "[-0.40422462  0.68904459 -2.26047547 -1.75138976]\n",
      "[-0.40422462  0.68904459 -2.26047547 -1.75138976] 4 3 1\n",
      "[ 1.47430016  0.76587985  0.42192568  0.71451707]\n",
      "[ 1.47430016  0.76587985  0.42192568  0.71451707] 4 3 1\n",
      "[ 0.55913888  0.8401503   0.18674029 -0.32851111]\n",
      "[ 0.55913888  0.8401503   0.18674029 -0.32851111] 4 3 1\n",
      "[-0.72109619  0.56129933 -0.25896416 -0.16632339]\n",
      "[-0.72109619  0.56129933 -0.25896416 -0.16632339] 4 3 1\n",
      "[ 1.23126402  1.05353349 -0.13316953 -0.18977998]\n",
      "[ 1.23126402  1.05353349 -0.13316953 -0.18977998] 4 3 1\n",
      "[-2.45278132  0.69625867  0.48153432  0.91262888]\n",
      "[-2.45278132  0.69625867  0.48153432  0.91262888] 4 3 1\n",
      "[-1.55859677 -0.26656714 -0.11958638  0.96835093]\n",
      "[-1.55859677 -0.26656714 -0.11958638  0.96835093] 4 3 1\n",
      "[ 1.66494591 -0.64786835 -1.08102822  0.04392952]\n",
      "[ 1.66494591 -0.64786835 -1.08102822  0.04392952] 4 3 1\n",
      "[ 0.30141198 -0.20658564 -0.04246956 -2.09161426]\n",
      "[ 0.30141198 -0.20658564 -0.04246956 -2.09161426] 4 3 1\n",
      "[-2.83804351  1.27529303  1.14948085 -0.05894355]\n",
      "[-2.83804351  1.27529303  1.14948085 -0.05894355] 4 3 1\n",
      "[ 0.09021319  1.08443145 -0.83429923 -1.3192176 ]\n",
      "[ 0.09021319  1.08443145 -0.83429923 -1.3192176 ] 4 3 1\n",
      "[ 1.36507477  0.22497759  0.37987163  1.38727224]\n",
      "[ 1.36507477  0.22497759  0.37987163  1.38727224] 4 3 1\n",
      "[-0.09169312  0.67627347 -0.62657946 -0.18902558]\n",
      "[-0.09169312  0.67627347 -0.62657946 -0.18902558] 4 3 1\n",
      "[-1.72774945  1.64948735  1.4020208   0.33116195]\n",
      "[-1.72774945  1.64948735  1.4020208   0.33116195] 4 3 1\n",
      "[-0.43105106 -0.24540923  1.56736868  0.24035922]\n",
      "[-0.43105106 -0.24540923  1.56736868  0.24035922] 4 3 1\n",
      "[-0.87478594  2.24664628 -0.01890323  0.31209147]\n",
      "[-0.87478594  2.24664628 -0.01890323  0.31209147] 4 3 1\n",
      "[-0.399832   -0.92734914  0.76651054 -0.4487624 ]\n",
      "[-0.399832   -0.92734914  0.76651054 -0.4487624 ] 4 3 1\n",
      "[-0.10344779  0.7774305   0.5126041   1.64198928]\n",
      "[-0.10344779  0.7774305   0.5126041   1.64198928] 4 3 1\n",
      "Iteration  0. Episode mean reward: -1193543.655\n",
      "[[-0.37030113  0.24325921 -1.28778306 -2.97953413]]\n",
      "[-0.37030113  0.24325921 -1.28778306 -2.97953413] 4 3 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-3f4cc89443d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeterministicContinuousActionLinearPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'theta_mean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mdo_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0mwritefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agent-%.4i.pkl'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Write out the env at the end so we store the parameters of this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import json, sys, os\n",
    "from os import path\n",
    "\n",
    "def cem(f, th_mean, batch_size, n_iter, elite_frac, initial_std=1.0):\n",
    "    \"\"\"\n",
    "    Generic implementation of the cross-entropy method for maximizing a black-box function\n",
    "\n",
    "    f: a function mapping from vector -> scalar\n",
    "    th_mean: initial mean over input distribution\n",
    "    batch_size: number of samples of theta to evaluate per batch\n",
    "    n_iter: number of batches\n",
    "    elite_frac: each batch, select this fraction of the top-performing samples\n",
    "    initial_std: initial standard deviation over parameter vectors\n",
    "    \"\"\"\n",
    "    n_elite = int(np.round(batch_size*elite_frac))\n",
    "    th_std = np.ones_like(th_mean) * initial_std\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        ths = np.array([th_mean + dth for dth in th_std[None,:] * np.random.randn(batch_size, th_mean.size)])\n",
    "        ys = np.array([f(th) for th in ths])\n",
    "        elite_inds = ys.argsort()[::-1][:n_elite]\n",
    "        elite_ths = ths[elite_inds]\n",
    "        th_mean = elite_ths.mean(axis=0)\n",
    "        th_std = elite_ths.std(axis=0)\n",
    "        yield {'ys' : ys, 'theta_mean' : th_mean, 'y_mean' : ys.mean()}\n",
    "\n",
    "def do_rollout(agent, env, num_steps, render=False):\n",
    "    total_rew, t = 0, 0\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "#     for t in range(num_steps):\n",
    "        a = agent.act(ob)\n",
    "        (ob, reward, done, _info) = env.step(a)\n",
    "        total_rew += reward\n",
    "        t += 1\n",
    "#         if render and t%3==0: env.render()\n",
    "#         if done: break\n",
    "    return total_rew, t+1\n",
    "\n",
    "# env = gym.make('bs-v3')\n",
    "# env.seed(0)\n",
    "# np.random.seed(0)\n",
    "params = dict(n_iter=10, batch_size=25, elite_frac = 0.2)\n",
    "num_steps = 200\n",
    "\n",
    "# You provide the directory to write to (can be an existing\n",
    "# directory, but can't contain previous monitor results. You can\n",
    "# also dump to a tempdir if you'd like: tempfile.mkdtemp().\n",
    "# outdir = '/tmp/cem-agent-results'\n",
    "# env = gym.wrappers.Monitor(env, outdir, force=True)\n",
    "\n",
    "# Prepare snapshotting\n",
    "# ----------------------------------------\n",
    "def writefile(fname, s):\n",
    "    with open(path.join('/tmp/cem-agent-results/', fname), 'w') as fh: fh.write(s)\n",
    "info = {}\n",
    "info['params'] = params\n",
    "info['env_id'] = env.spec.id\n",
    "# ------------------------------------------\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    print(theta)\n",
    "    agent = DeterministicContinuousActionLinearPolicy(theta, env.observation_space, env.action_space)\n",
    "    rew, T = do_rollout(agent, env, num_steps)\n",
    "    return rew\n",
    "\n",
    "# Train the agent, and snapshot each stage\n",
    "for (i, iterdata) in enumerate(cem(noisy_evaluation, np.zeros(env.observation_space.shape[0]+1), **params)):\n",
    "    print('Iteration %2i. Episode mean reward: %7.3f'%(i, iterdata['y_mean']))\n",
    "    print(iterdata['theta_mean'])\n",
    "    agent = DeterministicContinuousActionLinearPolicy(iterdata['theta_mean'][0], env.observation_space, env.action_space)\n",
    "    do_rollout(agent, env, 200, render=False)\n",
    "    writefile('agent-%.4i.pkl'%i, str(pickle.dumps(agent, -1)))\n",
    "\n",
    "# Write out the env at the end so we store the parameters of this\n",
    "# environment.\n",
    "writefile('info.json', json.dumps(info))\n",
    "\n",
    "env.close()\n",
    "\n",
    "#     logger.info(\"Successfully ran cross-entropy method. Now trying to upload results to the scoreboard. If it breaks, you can always just try re-uploading the same results.\")\n",
    "#     gym.upload(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.11486329e+06,  -2.11043706e+06,  -1.98097114e+06,\n",
       "        -1.96668867e+06,  -1.85924434e+06,  -1.83225317e+06,\n",
       "        -1.78758212e+06,  -1.76561868e+06,  -1.76444708e+06,\n",
       "        -1.46047481e+06,  -1.45153023e+06,  -1.40418020e+06,\n",
       "        -1.38885617e+06,  -1.08677060e+06,  -9.93212842e+05,\n",
       "        -8.49137387e+05,  -8.43171633e+05,  -7.90175585e+05,\n",
       "        -7.70819880e+05,  -5.80217897e+05,  -5.44619892e+05,\n",
       "        -5.11679655e+05,  -2.66302286e+05,   1.34923719e+02,\n",
       "         2.84528324e+05])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterdata['ys'].flatten()[iterdata['ys'].flatten().argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 10, 19, 12, 16, 20, 15, 14, 13, 18, 21, 17, 23, 22,  8,  6,  7,\n",
       "        5,  9,  4,  3,  2, 24,  0,  1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterdata['ys'].flatten().argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterdata['ys'].argsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iterdata['theta_mean'][0]) == (env.observation_space.shape[0] + 1) * env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0] + env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "unpickling stack underflow",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a401c0bad2fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/tmp/cem-agent-results/agent-0099.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m: unpickling stack underflow"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "with open('/tmp/cem-agent-results/agent-0099.pkl', 'rb') as f: x = pickle.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.43051544, -0.20901409])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.random.randn(3+1, 2), axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}